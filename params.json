{"name":"Cetune","tagline":"","body":"####CeTune: A Ceph Profiling and Tuning Framework\r\n\r\n######Functionality Description\r\n- CeTune is a toolkit/framework to deploy, benchmark, profile and tune *Ceph cluster performance. \r\n- Aim to speed up the procedure of benchmarking *Ceph performance, and provide clear data charts of system metrics, latency breakdown data for users to analyze *Ceph performance.\r\n- CeTune provides test performance through three interfaces: block, file system and object to evaluate *Ceph.\r\n\r\n######Maintainance\r\n- CeTune is an opensource project, under LGPL V2.1, Drived by INTEL BDT CSE team.\r\n- Maillist: https://github.com/01org/CeTune\r\n- Subscribe maillist: https://lists.01.org/mailman/listinfo/cephperformance\r\n\r\n\r\n######Quick Start\r\n- Prepare:\r\n  - one node as CeTune controller(AKA head), Other nodes as CeTune worker(AKA worker)\r\n  - Head is able to autossh to all workers include himself, head has a 'hosts' file contains all workers info.\r\n  - All nodes are able to connect to yum/apt-get repository and also being able to wget/curl from ceph.com\r\n  - package pre-installation:\r\n    - Install to head:\r\n      - apt-get/yum install -y python-pip pdsh unzip expect sysstat curl openjdk-7-jre haproxy python-matplotlib python-numpy\r\n      - pip install ceph-deploy pyyaml argparse\r\n    - Install to workers\r\n      - apt-get/yum install -y python-pip unzip sysstat curl openjdk-7-jre haproxy\r\n\r\n- Configure:\r\n  - conf/all.conf\r\n    - This is a configuration file to describe cluster, benchmark\r\n  - conf/tuner.yaml\r\n    - This is a configuration file to tune ceph cluster, including pool configuration, ceph.conf, disk tuning, etc.\r\n  - conf/cases.conf\r\n  -   This is a configuration file to decide which test case to run.\r\n\r\n- Executive workflow:\r\n  - CeTune WEBUI\r\n    - cd webui; python webui.py\r\n    - enter CeTune WebUI by Browser: http://{CeTune_server_ip}:8080\r\n\r\n  - CeTune CLI\r\n    - CeTune \"Deploy ceph cluster\" phase:\r\n      - CeTune will check if ceph is installed, if no, install ceph firstly, config version in all.conf\r\n      - CeTune will deploy ceph cluster according to cluster configuration in all.conf\r\n      - CeTune will apply tuning in tuner.yaml to ceph cluster, waiting ceph to be healthy\r\n      - How to kick off CeTune deploy\r\n      ```\r\n      config tuner.yaml workstages as \"deploy\" only\r\n      cd tuner; python tuner.py\r\n      ```\r\n     - CeTune \"Benchmark ceph cluster\" phase:\r\n       - CeTune will compare current ceph tuning with tuner.yaml, if tuner.yaml contains some ceph configuartion not applyed to ceph cluster yet, CeTune will re-generate ceph.conf and restart ceph cluster.\r\n       - Run benchmark lists in cases.conf, currently CeTune supported qemufio, rbdfio and cosbench.\r\n       - Start analyze results after done benchmarking, generating html after analyzer.\r\n       - How to kick off CeTune Benchmark\r\n       ```\r\n       generate test case from all.conf, and edit cases.conf manually to remove the test case if unnecessary.\r\n       config tuner.yaml workstages as \"benchmark\" only\r\n       cd tuner; python tuner.py\r\n       ```\r\n- User Guidance:\r\n  - [CeTune Documents Download Url](https://github.com/01org/CeTune/blob/master/CeTune%20Document.pdf)\r\n\r\n- Examples:\r\n  ```\r\n  root@client01:/root#  cd /root/cetune/tuner\r\n  root@client01:/root/cetune/tuner# python tuner.py\r\n  [LOG]Check ceph version, reinstall ceph if necessary\r\n  [LOG]start to redeploy ceph\r\n  [LOG]ceph.conf file generated\r\n  [LOG]Shutting down mon daemon\r\n  [LOG]Shutting down osd daemon\r\n  [LOG]Clean mon dir\r\n  [LOG]Started to mkfs.xfs on osd devices\r\n  [LOG]mkfs.xfs for /dev/sda1 on aceph01\r\n  … …\r\n  [LOG]mkfs.xfs for /dev/sdf1 on aceph04\r\n  [LOG]Build osd.0 daemon on aceph01\r\n  … …\r\n  [LOG]Build osd.39 daemon on aceph01\r\n  [LOG]delete ceph pool rbd\r\n  [LOG]delete ceph pool data\r\n  [LOG]delete ceph pool metadata\r\n  [LOG]create ceph pool rbd, pg_num is 8192\r\n  [LOG]set ceph pool rbd size to 2\r\n  [WARNING]Applied tuning, waiting ceph to be healthy\r\n  [WARNING]Applied tuning, waiting ceph to be healthy\r\n  … …\r\n  [LOG]Tuning has been applied to ceph cluster, ceph is healthy now \r\n  RUNID: 36, Result dir: //mnt/data/36-80-seqwrite-4k-100-300-vdb\r\n  [LOG]Prerun_check: check if rbd volumes are initialized\r\n  [WARNING]Ceph cluster used data: 0.00KB, planed data: 3276800MB\r\n  [WARNING]rbd volume initialization not done\r\n  [LOG]80 RBD Images created\r\n  [LOG]create rbd volume vm attaching xml\r\n  [LOG]Distribute vdbs xml\r\n  [LOG]Attach rbd image to vclient1\r\n  … …\r\n  [LOG]Start to initialize rbd volumes\r\n  [LOG]FIO Jobs started on [‘vclient01’,’vclient02’, …. ‘vclient80’]\r\n  [WARN]160 fio job still running\r\n  … …\r\n  [LOG]RBD initialization complete\r\n  [LOG]Prerun_check: check if fio installed in vclient\r\n  [LOG]Prerun_check: check if rbd volume attached\r\n  [LOG]Prerun_check: check if sysstat installed\r\n  [LOG]Prepare_run: distribute fio.conf to vclient\r\n  [LOG]Benchmark start\r\n  [LOG]FIO Jobs started on [‘vclient01’,’vclient02’, …. ‘vclient80’]\r\n  [WARN]160 fio job still running\r\n  … …\r\n  [LOG]stop monitoring, and workload\r\n  [LOG]collecting data\r\n  [LOG]processing data\r\n  [LOG]creating html report\r\n  [LOG]scp to result backup server\r\n  ```\r\n  - Result dir\r\n  ```\r\n  - 278-140-qemurbd-seqwrite-64k-qd64-40g-100-400-vdb.html\r\n  - conf\r\n    - all.conf\r\n\t  - tuner.yaml\r\n\t  - cetune_proces.log\r\n  - raw\r\n    - aceph01\r\n    - aceph02\r\n    - ...\r\n  ```\r\n  - Result HTML pages\r\n  ![CeTune HTML](https://github.com/01org/CeTune/blob/master/examples/CeTune.png)\r\n","google":"UA-69358033-1","note":"Don't delete this file! It's used internally to help with page regeneration."}